# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1scFM4Gbw1q5G19k45IBDAaTUKKXZvBe_
"""

!pip install pytesseract PyPDF2 pdf2image PyMuPDF python-docx gradio reportlab sentence-transformers scikit-learn scikit-fuzzy seaborn fpdf rank_bm25 mcdm

!sudo apt install tesseract-ocr -y
!pip install pytesseract PyPDF2 pdf2image PyMuPDF python-docx gradio reportlab sentence-transformers scikit-learn scikit-fuzzy seaborn fpdf rank_bm25 mcdm

from sentence_transformers import SentenceTransformer, util, CrossEncoder
import PyPDF2
import io
import docx
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import random
import pandas as pd
from fpdf import FPDF
import tempfile
import os
from rank_bm25 import BM25Okapi
from mcdm import rank

# PyMuPDF for OCR fallback, often imported as 'fitz'
try:
    import fitz # PyMuPDF
except ImportError:
    print("PyMuPDF (fitz) not found. OCR fallback for PDFs will not work.")


# Load SBERT model once
model = SentenceTransformer("all-MiniLM-L6-v2")

# Load Cross-Encoder model once (VERY accurate)
cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# ============================================================
# 3. TEXT EXTRACTION (PDF, DOCX, TXT) + OCR FALLBACK
# ============================================================
def extract_text(filename, file_bytes):

    # ------------------------
    # Try text-based PDF first
    # ------------------------
    if filename.lower().endswith(".pdf"):
        try:
            reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))
            text = ""
            for page in reader.pages:
                if page.extract_text():
                    text += page.extract_text() + "\n"
            if len(text.strip()) > 20:
                return text
        except:
            pass  # fall back to OCR

        # ------------------------
        # OCR fallback using PyMuPDF
        # ------------------------
        try:
            text = ""
            pdf_doc = fitz.open(stream=file_bytes, filetype="pdf")
            for page in pdf_doc:
                text += page.get_text("text")
            if len(text.strip()) > 10:
                return text
        except:
            return ""

    # ------------------------
    # DOCX
    # ------------------------
    if filename.lower().endswith(".docx"):
        try:
            docu = docx.Document(io.BytesIO(file_bytes))
            return "\n".join([p.text for p in docu.paragraphs])
        except:
            return ""

    # ------------------------
    # TXT
    # ------------------------
    if filename.lower().endswith(".txt"):
        try:
            return file_bytes.decode("utf-8")
        except:
            return ""

    return ""


# =====================
# 4. SKILL EXTRACTION
# =====================

def extract_skills(text):
    """
    Extract skills exactly as written in the resume.
    Priority:
    1. Explicit Skills section
    2. Line-based technical phrases fallback
    """

    if not text or len(text.strip()) < 10:
        return []

    text = text.lower()

    # -----------------------------
    # 1. TRY TO FIND SKILLS SECTION
    # -----------------------------
    skill_headers = [
        "skills",
        "technical skills",
        "core competencies",
        "key skills",
        "expertise",
        "professional skills"
    ]

    pattern = r"(" + "|".join(skill_headers) + r")\s*[:\-]?\s*(.*?)(\n\n|\Z)"
    match = re.search(pattern, text, re.DOTALL)

    skills = []

    if match:
        skill_block = match.group(2)

        # split by commas, bullets, pipes, or newlines
        raw_skills = re.split(r",|\n|‚Ä¢|\||\-", skill_block)

        skills = [
            s.strip()
            for s in raw_skills
            if 2 < len(s.strip()) < 50
        ]

        if skills:
            return list(set(skills))

    # ------------------------------------
    # 2. FALLBACK: TECH-LIKE PHRASE LINES
    # ------------------------------------
    fallback_skills = []

    for line in text.splitlines():
        line = line.strip()

        # heuristics: short, noun-like, not a sentence
        if (
            3 < len(line) < 60 and
            line.count(" ") <= 5 and
            not line.endswith(".") and
            not any(x in line for x in ["@", "www", "http"])
        ):
            fallback_skills.append(line)

    return list(set(fallback_skills[:20]))  # cap to avoid noise


# ============================================================
# 5. UNIVERSAL FILE READER FOR GRADIO
# ============================================================

def read_file(f):
    """Handles ALL Gradio file formats safely."""
    if hasattr(f, "read"):
        return f.read()
    if hasattr(f, "data"):
        return f.data()
    if hasattr(f, "name"):
        with open(f.name, "rb") as fp:
            return fp.read()
    return f  # fallback

# ============================================================
# LARGE LANGUAGE MODEL (LLM)
# ============================================================

INTERVIEW_BANK = {
    "python": [
        ("What is the output of len([1,2,3])?", ["4","3","2","Error"], 1),
        ("What is a lambda function?",
         ["Anonymous function","Class method","Python library","Data type"], 0),
    ],
    "machine learning": [
        ("Which algorithm is used for classification?",
         ["K-Means","Linear Regression","Decision Tree","PCA"], 2),
        ("What does overfitting mean?",
         ["Model too simple","Model memorizes training data",
          "Too much unlabeled data","Bad optimizer"], 1),
    ],
    "deep learning": [
        ("What is a neural network?",
         ["Linear model","Rule-based system","Graph of layers","Database"], 2),
        ("Which library is used for DL?", ["NumPy","Pandas","TensorFlow","matplotlib"], 2),
    ],
    "nlp": [
        ("What is tokenization?", ["Model training","Text splitting","OCR","Embedding"], 1),
        ("Which model is for embeddings?",
         ["ResNet","BERT","VGG16","YOLO"], 1),
    ]
}


def generate_interview_questions(skills, num_q=5):
    questions = []
    pool = []

    # choose from skill-related questions
    for skill in skills:
        if skill in INTERVIEW_BANK:
            pool.extend(INTERVIEW_BANK[skill])

    # fallback if no skill-based questions found
    if len(pool) == 0:
        pool.extend(INTERVIEW_BANK["python"])
        pool.extend(INTERVIEW_BANK["machine learning"])

    random.shuffle(pool)
    chosen = pool[:num_q]

    formatted = []
    for q, opts, ans in chosen:
        formatted.append({
            "question": q,
            "options": opts,
            "answer": ans
        })

    return formatted


# ============================================================
# 6. MAIN PROCESSING PIPELINE
# ============================================================
def process(job_description, files):
    log = []

    try:
        # --------------------------------------------
        # LOAD & EXTRACT RESUMES
        # --------------------------------------------
        candidate_resumes = []

        for f in files:
            if f is None:
                continue

            if "report" in f.name.lower():
                log.append(f"Skipping report (not a resume): {f.name}")
                continue

            data = read_file(f)
            text = extract_text(f.name, data)

            if len(text.strip()) < 10:
                log.append(f"[WARNING] {f.name} produced <10 chars via OCR. Keeping anyway.")
                text = "empty resume text"

            skills = extract_skills(text)

            import os
            clean_name = os.path.basename(f.name)

            candidate_resumes.append({
               "id": clean_name,
                "resume_text": text,
               "skills": skills
            })

        if len(candidate_resumes) == 0:
            return (
                pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), None, None, None, None,
                "‚ùå No valid resumes."
            )

        df = pd.DataFrame(candidate_resumes)
        log.append(f"Loaded {len(df)} resumes.")


        # --------------------------------------------
        # TF-IDF SIMILARITY
        # --------------------------------------------
        try:
            docs = [job_description] + df["resume_text"].tolist()
            tfidf = TfidfVectorizer(stop_words="english")
            tfidf_matrix = tfidf.fit_transform(docs)
            tfidf_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]
        except:
            tfidf_scores = np.zeros(len(df))
            log.append("TF-IDF failed ‚Üí using zeros")

        df["tfidf_similarity"] = tfidf_scores


        # --------------------------------------------
        # SBERT SIMILARITY
        # --------------------------------------------
        try:
            job_emb = model.encode(job_description, convert_to_tensor=True)
            sbert_list = []
            for txt in df["resume_text"]:
                try:
                    emb = model.encode(txt, convert_to_tensor=True)
                    sbert_list.append(util.cos_sim(job_emb, emb).item())
                except:
                    sbert_list.append(0)
            df["sbert_similarity"] = sbert_list
        except:
            df["sbert_similarity"] = np.zeros(len(df))
            log.append("SBERT failed ‚Üí using zeros")


        # --------------------------------------------
        # BM25 SCORE
        # ============================================
        try:
            tokenized_resumes = [c.lower().split() for c in df["resume_text"]]
            bm25 = BM25Okapi(tokenized_resumes)

            query_tokens = job_description.lower().split()
            bm25_scores = bm25.get_scores(query_tokens)
            bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))

            df["bm25_score"] = bm25_scores
            log.append("BM25 score computed successfully.")
        except Exception as e:
            df["bm25_score"] = np.zeros(len(df))
            log.append(f"BM25 scoring failed ‚Üí using zeros. Error: {e}")


        # --------------------------------------------
        # CROSS-ENCODER (VERY ACCURATE MATCHING)
        # --------------------------------------------
        try:
            ce_pairs = [(job_description, txt) for txt in df["resume_text"].tolist()]

            # raw model scores
            ce_scores = cross_encoder.predict(ce_pairs)

            # convert to probability (0‚Äì1)
            ce_prob = 1 / (1 + np.exp(-ce_scores))

            df["cross_score_raw"] = ce_scores
            df["cross_score_prob"] = ce_prob

        except:
            df["cross_score_raw"] = np.zeros(len(df))
            df["cross_score_prob"] = np.zeros(len(df))
            log.append("Cross-Encoder failed ‚Üí using zeros")



        # --------------------------------------------
        # SMART FUZZY SCORE
        # --------------------------------------------
        try:
            # Simple keyword overlap between job description and resume text
            jd_tokens = set(job_description.lower().split())
            fuzzy_scores = []

            for txt in df["resume_text"]:
                resume_tokens = set(txt.lower().split())
                overlap = jd_tokens.intersection(resume_tokens)
                # ratio of overlap to job description length
                score = len(overlap) / (len(jd_tokens) + 1e-6)
                # scale to 40‚Äì90 range
                scaled = int(40 + score * 50)
                fuzzy_scores.append(scaled)

            df["fuzzy_score"] = fuzzy_scores
            log.append("Fuzzy score computed based on requirement synchronization.")
        except Exception as e:
            df["fuzzy_score"] = np.random.randint(40, 90, size=len(df))
            log.append(f"Fuzzy score failed ‚Üí using random. Error: {e}")



        # --------------------------------------------
        # HYBRID FINAL SCORE
        # --------------------------------------------
        def norm(col):
            if col.max() == col.min():
                return np.ones(len(col)) * 0.5
            return (col - col.min()) / (col.max() - col.min())

        df["hybrid_score"] = (
            0.0777 * norm(df["tfidf_similarity"]) +
            0.501 * norm(df["sbert_similarity"]) +
            0.0426 * norm(df["fuzzy_score"]) +
            0.2344 * norm(df["cross_score_prob"]) +
            0.1443 * norm(df["bm25_score"])
        )

        final_rank = df.sort_values("hybrid_score", ascending=False).reset_index(drop=True)
        log.append("Ranking computed successfully.")


        # --------------------------------------------
        # SCORE TABLE FOR DISPLAY
        # --------------------------------------------
        score_table = final_rank[[
            "id",
            "tfidf_similarity",
            "sbert_similarity",
            "fuzzy_score",
            "bm25_score",
            "cross_score_prob",
            "hybrid_score"
        ]]


        # ===== VISUAL PLACEHOLDERS =====
        # Call the plotting functions here
        scatter = plot_scatter(df)
        heatmap = plot_heatmap(df)
        contrib = plot_contribution()
        # pdf_report will be defined in the next section


        # --------------------------------------------
        # PDF REPORT GENERATION
        # --------------------------------------------
        try:
            pdf = FPDF()
            pdf.add_page()
            pdf.set_font("Arial", size=12)

            pdf.cell(200, 10, txt="Recruitment Ranking Report", ln=True, align="C")
            pdf.ln(10)

            # Job description
            pdf.multi_cell(0, 10, txt=f"Job Description:\n{job_description}")
            pdf.ln(10)

            # Ranking table
            pdf.cell(200, 10, txt="Final Candidate Ranking", ln=True, align="L")
            pdf.ln(5)

            for idx, row in final_rank.iterrows():
                pdf.multi_cell(
                    0, 10,
                    txt=f"{idx+1}. {row['id']} | Hybrid Score: {row['hybrid_score']:.3f} | "
                        f"TF-IDF: {row['tfidf_similarity']:.3f}, SBERT: {row['sbert_similarity']:.3f}, "
                        f"Fuzzy: {row['fuzzy_score']}, BM25: {row['bm25_score']:.3f}"
                )
                pdf.ln(2)

            # Save to temp file
            tmpfile = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
            pdf.output(tmpfile.name)
            pdf_report = tmpfile.name
            log.append("PDF report generated successfully.")
        except Exception as e:
            pdf_report = None
            log.append(f"PDF generation failed: {e}")

        # --------------------------------------------
        # FINAL RETURN ‚Äî ***ALL 8 OUTPUTS***
        # --------------------------------------------
        return (
            final_rank,                  # 1 Ranking
            score_table,                 # 2 Model scores
            df[["id","skills"]],         # 3 Skills
            scatter,                     # 4 Plot 1
            heatmap,                     # 5 Plot 2
            contrib,                     # 6 Plot 3
            pdf_report,                  # 7 PDF
            "\n".join(log)               # 8 Debug log
        )

    except Exception as e:
        return (
            pd.DataFrame(), pd.DataFrame(), pd.DataFrame(),
            None, None, None, None,
            f"‚ùå Crash: {e}"
        )

import matplotlib.pyplot as plt
import seaborn as sns

def plot_scatter(df):
    """Generates scatter plots comparing multiple metrics against SBERT similarity."""
    fig, axes = plt.subplots(3, 2, figsize=(10, 7))

    scatter_pairs = [
        ('tfidf_similarity', 'TF-IDF vs Hybrid'),
        ('bm25_score', 'BM25 vs Hybrid'),
        ('cross_score_prob', 'Cross-Encoder Probability vs Hybrid'),
        ('sbert_similarity', 'S-BERT vs Hybrid'),
        ('fuzzy_score', 'Fuzzy vs Hybrid')
    ]

    for ax, (metric, title) in zip(axes.flatten(), scatter_pairs):
        sns.scatterplot(data=df, x=metric, y='hybrid_score', ax=ax)
        ax.set_title(title)
        ax.set_xlabel(metric)
        ax.set_ylabel('Hybrid Score')
        ax.grid(True)

    plt.tight_layout()
    return fig


def plot_heatmap(df):
    """Generates a heatmap of correlation between scoring metrics."""
    metrics = [
        'tfidf_similarity',
        'sbert_similarity',
        'fuzzy_score',
        'bm25_score',
        'cross_score_prob',
        'hybrid_score'
    ]

    # Filter only existing columns to avoid errors
    metrics = [m for m in metrics if m in df.columns]

    corr_matrix = df[metrics].corr()

    fig = plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Correlation Matrix of Scoring Metrics')
    return fig


def plot_contribution():
    """Generates a bar plot showing the contribution of each metric to the hybrid score."""
    weights = {
        'TF-IDF': 0.0777,
        'SBERT': 0.501,
        'Cross-Encoder': 0.2344,
        'Fuzzy': 0.0426,
        'BM25': 0.1443
    }

    fig = plt.figure(figsize=(8, 5))
    plt.bar(weights.keys(), weights.values(),
            color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'orange'])
    plt.title('Contribution of Metrics to Hybrid Score')
    plt.xlabel('Metric')
    plt.ylabel('Weight')
    plt.ylim(0, 0.5)
    return fig


print("Updated plotting functions")

import gradio as gr
import pandas as pd


# ============================================================
# Fix: dropdown update helper
# ============================================================
def update_candidate_dropdown(rank_df):
    """Update candidate dropdown after ranking."""
    try:
        if rank_df is None or isinstance(rank_df, str) or rank_df.empty:
            return gr.Dropdown(choices=[], value=None)

        if not isinstance(rank_df, pd.DataFrame):
            # This case might happen if the output is a list of lists from a dataframe, convert it back
            # Assuming rank_df is a list of lists where the first sublist is headers
            if isinstance(rank_df, list) and len(rank_df) > 1:
                rank_df = pd.DataFrame(rank_df[1:], columns=rank_df[0])
            else:
                return gr.Dropdown(choices=[], value=None)

        if "id" not in rank_df.columns:
            return gr.Dropdown(choices=[], value=None)

        candidates = rank_df["id"].astype(str).tolist()
        return gr.Dropdown(choices=candidates, value=candidates[0])

    except Exception as e:
        print(f"Error updating dropdown: {e}")
        return gr.Dropdown(choices=[], value=None)



# ============================================================
# MCQ generator helper
# ============================================================
def generate_interview_questions(skills):
    if not skills:
        return {"message": "No skills found."}

    base = ["Not related", "Basic knowledge", "No idea"]
    out = []

    for skill in skills[:5]:
        out.append({
            "question": f"What is {skill}?",
            "options": [
                f"A concept used in {skill}",
                f"A tool/framework used in {skill}",
                *base
            ],
            "correct_index": 0
        })

    return out


def build_questions(candidate_id, skills_df):
    if not candidate_id:
        return {"message": "Select a candidate first."}

    if not isinstance(skills_df, pd.DataFrame):
        # This case might happen if the output is a list of lists from a dataframe, convert it back
        # Assuming skills_df is a list of lists where the first sublist is headers
        if isinstance(skills_df, list) and len(skills_df) > 1:
            skills_df = pd.DataFrame(skills_df[1:], columns=skills_df[0])
        else:
            return {"message": "Skills data format error.", "skills_data": skills_df}

    row = skills_df[skills_df["id"] == candidate_id]
    if row.empty:
        return {"message": "Skills not found."}

    skills = row["skills"].iloc[0]

    if isinstance(skills, str):
        # convert "['python', 'sql']" to list
        import ast
        try:
            skills = ast.literal_eval(skills)
            if not isinstance(skills, list): # Ensure it's a list after eval
                skills = [s.strip(" '\"") for s in skills.strip("[]").split(",")] # Fallback for malformed string
        except (ValueError, SyntaxError):
            skills = [s.strip(" '\"") for s in skills.strip("[]").split(",")] # Manual parse

    return generate_interview_questions(skills)



# ============================================================
# GRADIO APP UI
# ============================================================
with gr.Blocks(title="AI Resume Ranker") as app:

    gr.Markdown("## Upload resumes ‚Üí extract skills ‚Üí rank candidates ‚Üí interview ‚Üí download report")

    # Inputs
    with gr.Row():
        job_input = gr.Textbox(label="Job Description", lines=4)
        file_input = gr.File(label="Upload Resumes", file_count="multiple")

    run_btn = gr.Button("Process Resumes", variant="primary")

    # Tabs
    with gr.Tabs():
        with gr.TabItem("üèÜ Ranking"):
            out_rank = gr.Dataframe(label="Final Ranking")

        with gr.TabItem("üìê Model Scores"):
            out_scores = gr.Dataframe(label="Model Score Breakdown")

        with gr.TabItem("üß© Extracted Skills"):
            out_skills = gr.Dataframe(label="Skills per Resume")

        with gr.TabItem("üìä Visualizations"):
            out_plot1 = gr.Plot()
            out_plot2 = gr.Plot()
            out_plot3 = gr.Plot()

        with gr.TabItem("üì• Download Report"):
            out_pdf = gr.File(label="Generated Report")

        with gr.TabItem("üêû Debug Log"):
            out_log = gr.Textbox(label="Log", lines=15)


        # ============================================================
        # INTERVIEW MODULE
        # ============================================================
        with gr.TabItem("üó£Ô∏è Interview Module"):
            gr.Markdown("### Automatically generate MCQ interview questions")

            candidate_select = gr.Dropdown(
                label="Select Candidate",
                choices=[],
                value=None
            )

            gen_q_btn = gr.Button("Generate Interview Questions")
            interview_output = gr.JSON(label="Generated MCQs")


    # ============================================================
    # BUTTON CONNECTIONS
    # ============================================================

    # MUST MATCH EXACT ORDER OF process() return values
    run_btn.click(
        process,
        inputs=[job_input, file_input],
        outputs=[
            out_rank,      # index 0
            out_scores,
            out_skills,
            out_plot1,
            out_plot2,
            out_plot3,
            out_pdf,
            out_log
        ]
    ).then(
        update_candidate_dropdown,   # receives out_rank correctly
        inputs=out_rank,
        outputs=candidate_select
    )

    gen_q_btn.click(
        build_questions,
        inputs=[candidate_select, out_skills],
        outputs=interview_output
    )


app.launch(debug=True)

app.launch()
